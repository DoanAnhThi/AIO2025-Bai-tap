{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import các thư viện cần thiết:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "nltk.download('stopwords ')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tải bộ dữ liệu: Các bạn tải bộ dữ liệu tại đây. Ở đây, ta sẽ chỉ quan tâm đến file\n",
    "all_data.csv. Dưới đây là thông tin 4 hàng đầu tiên của bảng dữ liệu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Đọc bộ dữ liệu: Sử dụng thư viện pandas, các bạn đọc bộ dữ liệu lên như sau, lưu ý cần\n",
    "phải cài đặt tham số encoding='ISO-8859-1' để đưa về định dạng đúng như đoạn code như\n",
    "sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/all -data.csv'\n",
    "headers = ['sentiment ', 'content ']\n",
    "df = pd.read_csv(\n",
    "dataset_path ,\n",
    "names=headers ,\n",
    "encoding='ISO -8859 -1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ đây, ta có thể xác định được danh sách các class của bài toán bằng phương thức unique()\n",
    "của Pandas, đồng thời ta cũng sẽ đổi class dạng string thành số nguyên đại diện cho ID của\n",
    "chúng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "class_name: idx for idx , class_name in enumerate(df['sentiment ']. unique ().tolist ())\n",
    "}\n",
    "df['sentiment '] = df['sentiment ']. apply(lambda x: classes[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Tiền xử lý dữ liệu: Để tận dùng hàm apply của pandas, ta sẽ tiền xử lý văn bản cột\n",
    "content ngay tại đây. Đầu tiên, định nghĩa hàm chuẩn hóa, nhận tham số đầu vào là 1 text\n",
    "và trả về 1 text đã được chuẩn hóa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english ')\n",
    "stemmer = PorterStemmer ()\n",
    "\n",
    "def text_normalize(text):\n",
    "text = text.lower()\n",
    "text = unidecode.unidecode(text)\n",
    "text = text.strip()\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text = ' '.join([word for word in text.split(' ') if word not inenglish_stop_words ])\n",
    "text = ' '.join([ stemmer.stem(word) for word in text.split(' ')])\n",
    "\n",
    "return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Xây dựng bộ từ vựng: Để huấn luyện dữ liệu văn bản, ta cần chuyển đổi các văn bản\n",
    "thành vector. Phương thức đơn giản nhất, ta sẽ thu thập toàn bộ các từ trong bộ dữ liệu\n",
    "thành một tập từ vựng, mỗi từ có một ID riêng. Từ đó, với một văn bản đầu, ta quy đổi\n",
    "sang ID tương ứng, như vậy sẽ được một vector số nguyên. Đoạn code tạo bộ từ vựng được\n",
    "thực hiện như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for sentence in df['content ']. tolist ():\n",
    "tokens = sentence.split ()\n",
    "for token in tokens:\n",
    "if token not in vocab:\n",
    "vocab.append(token)\n",
    "\n",
    "vocab.append('UNK')\n",
    "vocab.append('PAD')\n",
    "word_to_idx = {word: idx for idx , word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong đó, 'UNK' đại diện cho các từ không thuộc bộ từ vựng và 'PAD' đại diện cho các ô\n",
    "trống được thêm vào để thỏa mãn độ dài tối thiểu của một văn bản mà ta quy định về sau.\n",
    "Với bộ từ vựng trên, ta xây dựng hàm transform() dùng để chuyển đổi văn bản thành danh\n",
    "sách các số nguyên như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text , word_to_idx , max_seq_len):\n",
    "tokens = []\n",
    "for w in text.split():\n",
    "try:\n",
    "w_ids = word_to_idx[w]\n",
    "except:\n",
    "w_ids = word_to_idx['UNK']\n",
    "tokens.append(w_ids)\n",
    "\n",
    "if len(tokens) < max_seq_len:\n",
    "tokens += [word_to_idx['PAD']] * (max_seq_len - len(tokens))\n",
    "elif len(tokens) > max_seq_len:\n",
    "tokens = tokens [: max_seq_len]\n",
    "\n",
    "return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Chia bộ dữ liệu train, val, test: Ta dùng hàm train_test_split của thư viện scikit-learn\n",
    "để chia bộ dữ liệu thành 3 bộ train/val/test theo tỷ lệ 7:2:1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "test_size = 0.125\n",
    "is_shuffle = True\n",
    "texts = df['content ']. tolist ()\n",
    "labels = df['sentiment ']. tolist ()\n",
    "\n",
    "X_train , X_val , y_train , y_val = train_test_split(\n",
    "texts , labels ,\n",
    "test_size=val_size ,\n",
    "random_state=seed ,\n",
    "shuffle=is_shuffle\n",
    ")\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(\n",
    "X_train , y_train ,\n",
    "test_size=val_size ,\n",
    "random_state=seed ,\n",
    "shuffle=is_shuffle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Xây dựng pytorch datasets: Ta triển khai class tên FinancialNews với đầu vào gồm cặp\n",
    "dữ liệu đầu vào X - y, bộ từ điển cũng như hàm transform như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialNews(Dataset):\n",
    "def __init__(\n",
    "self ,\n",
    "X, y,\n",
    "word_to_idx ,\n",
    "max_seq_len ,\n",
    "transform=None\n",
    "):\n",
    "self.texts = X\n",
    "self.labels = y\n",
    "self.word_to_idx = word_to_idx\n",
    "self.max_seq_len = max_seq_len\n",
    "self.transform = transform\n",
    "\n",
    "def __len__(self):\n",
    "return len(self.texts)\n",
    "\n",
    "def __getitem__(self , idx):\n",
    "text = self.texts[idx]\n",
    "label = self.labels[idx]\n",
    "\n",
    "if self.transform:\n",
    "text = self.transform(\n",
    "text ,\n",
    "self.word_to_idx ,\n",
    "self.max_seq_len\n",
    ")\n",
    "text = torch.tensor(text)\n",
    "\n",
    "return text , label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Khai báo dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 32\n",
    "\n",
    "train_dataset = FinancialNews(\n",
    "X_train , y_train ,\n",
    "word_to_idx=word_to_idx ,\n",
    "max_seq_len=max_seq_len ,\n",
    "transform=transform\n",
    ")\n",
    "val_dataset = FinancialNews(\n",
    "X_val , y_val ,\n",
    "word_to_idx=word_to_idx ,\n",
    "max_seq_len=max_seq_len ,\n",
    "transform=transform\n",
    ")\n",
    "test_dataset = FinancialNews(\n",
    "X_test , y_test ,\n",
    "word_to_idx=word_to_idx ,\n",
    "max_seq_len=max_seq_len ,\n",
    "transform=transform\n",
    ")\n",
    "\n",
    "train_batch_size = 128\n",
    "test_batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "train_dataset ,\n",
    "batch_size=train_batch_size ,\n",
    "shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "val_dataset ,\n",
    "batch_size=test_batch_size ,\n",
    "shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "test_dataset ,\n",
    "batch_size=test_batch_size ,\n",
    "shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Xây dựng mô hình: Chúng ta sẽ xây dựng phân loại cảm xúc với 2 layer RNNs. Vector\n",
    "tại hidden state cuối cùng của RNN layer thứ 2 sẽ được đưa vào FC layer để thực hiện dự\n",
    "đoán. Các bạn có thể quan sát rõ hơn thông qua hình dưới đây:<br>\n",
    "Như vậy, code cài đặt như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "def __init__(\n",
    "self , vocab_size , embedding_dim ,\n",
    "hidden_size , n_layers , n_classes ,\n",
    "dropout_prob\n",
    "):\n",
    "super(SentimentClassifier , self).__init__ ()\n",
    "self.embedding = nn.Embedding(vocab_size , embedding_dim)\n",
    "self.rnn = nn.RNN(embedding_dim , hidden_size , n_layers , batch_first=True)\n",
    "self.norm = nn.LayerNorm(hidden_size)\n",
    "self.dropout = nn.Dropout(dropout_prob)\n",
    "self.fc1 = nn.Linear(hidden_size , 16)\n",
    "self.relu = nn.ReLU()\n",
    "self.fc2 = nn.Linear (16, n_classes)\n",
    "\n",
    "def forward(self , x):\n",
    "x = self.embedding(x)\n",
    "x, hn = self.rnn(x)\n",
    "x = x[:, -1, :]\n",
    "x = self.norm(x)\n",
    "x = self.dropout(x)\n",
    "x = self.fc1(x)\n",
    "x = self.relu(x)\n",
    "x = self.fc2(x)\n",
    "\n",
    "return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta định nghĩa class với tên SentimentClassifier, nhận tham số đầu vào gồm: kích thước bộ\n",
    "từ vựng (vocab_size), kích thước không gian vector của mỗi từ trong chuỗi đầu vào (em-\n",
    "bedding_dim), kích thước không gian vector của hidden state (hidden_size), số lượng RNN\n",
    "layer (n_layers), số class dự đoán của bài toán (n_classes) và tỉ lệ dropout (dropout_prob).\n",
    "Với class trên, ta khai báo mô hình SentimentClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(list(classes.keys()))\n",
    "embedding_dim = 64\n",
    "hidden_size = 64\n",
    "n_layers = 2\n",
    "dropout_prob = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available () else 'cpu'\n",
    "\n",
    "model = SentimentClassifier(\n",
    "vocab_size=vocab_size ,\n",
    "embedding_dim=embedding_dim ,\n",
    "hidden_size=hidden_size ,\n",
    "n_layers=n_layers ,\n",
    "n_classes=n_classes ,\n",
    "dropout_prob=dropout_prob\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Cài đặt hàm loss và optimizer: Ta sử dụng hàm loss crossentropy cho bài toán phân loại\n",
    "và Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "epochs = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss ()\n",
    "optimizer = torch.optim.Adam(\n",
    "model.parameters (),\n",
    "lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Thực hiện huấn luyện: Ta định nghĩa hàm fit() và evaluate() dùng để huấn luyện và đánh\n",
    "giá mô hình như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "model ,\n",
    "train_loader ,\n",
    "val_loader ,\n",
    "criterion ,\n",
    "optimizer ,\n",
    "device ,\n",
    "epochs\n",
    "):\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "batch_train_losses = []\n",
    "\n",
    "model.train()\n",
    "for idx , (inputs , labels) in enumerate(train_loader):\n",
    "inputs , labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "optimizer.zero_grad ()\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs , labels)\n",
    "loss.backward ()\n",
    "optimizer.step()\n",
    "\n",
    "batch_train_losses.append(loss.item())\n",
    "\n",
    "train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "val_loss , val_acc = evaluate(\n",
    "model , val_loader ,\n",
    "criterion , device\n",
    ")\n",
    "val_losses.append(val_loss)\n",
    "\n",
    "print(f'EPOCH {epoch + 1}:\\ tTrain loss: {train_loss :.4f}\\tVal loss: {val_loss :.4f}')\n",
    "\n",
    "return train_losses , val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model , dataloader , criterion , device):\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "losses = []\n",
    "with torch.no_grad ():\n",
    "for inputs , labels in dataloader:\n",
    "inputs , labels = inputs.to(device), labels.to(device)\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs , labels)\n",
    "losses.append(loss.item())\n",
    "_, predicted = torch.max(outputs.data , 1)\n",
    "total += labels.size (0)\n",
    "correct += (predicted == labels).sum().item()\n",
    "\n",
    "loss = sum(losses) / len(losses)\n",
    "acc = correct / total\n",
    "\n",
    "return loss , acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tổng hợp tất cả các dữ kiện trên, ta tiến hành huấn luyện mô hình SentimentClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses , val_losses = fit(\n",
    "model ,\n",
    "train_loader ,\n",
    "val_loader ,\n",
    "criterion ,\n",
    "optimizer ,\n",
    "device ,\n",
    "epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Đánh giá mô hình: Sử dụng hàm evaluate() đã xây dựng ở trên, ta đánh giá mô hình trên\n",
    "hai tập val và test như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss , val_acc = evaluate(\n",
    "model ,\n",
    "val_loader ,\n",
    "criterion ,\n",
    "device\n",
    ")\n",
    "test_loss , test_acc = evaluate(\n",
    "model ,\n",
    "test_loader ,\n",
    "criterion ,\n",
    "device\n",
    ")\n",
    "\n",
    "print('Evaluation on val/test dataset ')\n",
    "print('Val accuracy: ', val_acc)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Triển khai mô hình: Dựa trên mô hình đã huấn luyện và thư viện streamlit để triển khai\n",
    "ứng dụng:<br>\n",
    "– Link Streamlit<br>\n",
    "– Github<br>\n",
    "– Giao Diện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
